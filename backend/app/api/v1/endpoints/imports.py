"""
Import API endpoints for file upload and data import.
"""

import os
import uuid
import tempfile
import logging
from pathlib import Path
from typing import List
from datetime import datetime, timedelta

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, status
from sqlalchemy.orm import Session

from app.core.database import get_db

logger = logging.getLogger(__name__)
from app.models.import_job import ImportJob
from app.parsers.factory import FileParserFactory
from app.ai.factory import AIProviderFactory
from app.tasks.import_tasks import process_file_import
from app.services.import_service import ImportService
from app.services.device_detection_service import DeviceDetectionService
from app.services.product_type_learning_service import ProductTypeLearningService
from app.services.device_learning_service import DeviceLearningService
from app.services.size_learning_service import SizeLearningService
from app.services.supabase_service import SupabaseService
from app.schemas.import_job import (
    FileUploadRequest,
    FileUploadResponse,
    ImportJobCreateRequest,
    ImportJobResponse,
    ParsePreviewRequest,
    ParsePreviewResponse,
    ImportDataRequest,
    ImportDataResponse,
    ImportJobStatus,
)
from app.schemas.field_mapping import (
    STANDARD_FIELDS,
    AutoMappingResult,
    auto_map_columns,
)


router = APIRouter()


@router.post("/upload", response_model=FileUploadResponse)
async def upload_file(
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    """
    Upload file for import.
    Returns upload ID and presigned URL (or saves directly for simplicity).
    """
    try:
        # Validate file extension
        file_ext = Path(file.filename).suffix.lower()
        if not FileParserFactory.is_supported(Path(file.filename)):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Unsupported file format: {file_ext}"
            )

        # Generate unique upload ID
        upload_id = str(uuid.uuid4())

        # Create temp directory for uploads
        upload_dir = Path(tempfile.gettempdir()) / "accusync_uploads"
        upload_dir.mkdir(exist_ok=True)

        # Save file
        file_path = upload_dir / f"{upload_id}{file_ext}"
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)

        # In production, would use S3 presigned URL
        # For now, return local path info
        return FileUploadResponse(
            upload_id=upload_id,
            upload_url=str(file_path),
            expires_at=datetime.utcnow() + timedelta(hours=1)
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Upload failed: {str(e)}"
        )


@router.post("/preview", response_model=ParsePreviewResponse)
async def preview_parse(
    request: ParsePreviewRequest,
    db: Session = Depends(get_db)
):
    """
    Preview file parsing without creating import job.
    Shows first N rows to verify format.
    """
    try:
        # Reconstruct file path from upload_id
        upload_dir = Path(tempfile.gettempdir()) / "accusync_uploads"
        file_ext = Path(request.filename).suffix.lower()
        file_path = upload_dir / f"{request.upload_id}{file_ext}"

        if not file_path.exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Upload not found or expired"
            )

        # Create AI provider
        ai_provider = AIProviderFactory.create()

        # Create parser
        parser = FileParserFactory.create_parser(
            file_path=file_path,
            ai_provider=ai_provider
        )

        if parser is None:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Unsupported file format: {file_ext}"
            )

        # Parse file (limit rows for preview)
        import asyncio
        parser_options = request.parser_options or {}

        parse_result = await parser.parse(
            file_path=file_path,
            **parser_options
        )

        if not parse_result.success:
            return ParsePreviewResponse(
                success=False,
                columns=[],
                data=[],
                row_count=0,
                errors=parse_result.errors,
                warnings=parse_result.warnings
            )

        # Limit data for preview
        preview_data = parse_result.data[:request.preview_rows]

        # Extract keywords from product name for each row
        # æ©Ÿç¨®æ¤œå‡ºã¨ã‚µã‚¤ã‚ºæŠ½å‡ºã‚’å®Ÿè¡Œï¼ˆãƒ‡ã‚¶ã‚¤ãƒ³ãƒã‚¹ã‚¿ãƒ¼é€£æºã‚‚å«ã‚€ï¼‰
        device_detector = DeviceDetectionService(db)
        product_type_learning_service = ProductTypeLearningService(db)
        device_learning_service = DeviceLearningService(db)
        size_learning_service = SizeLearningService(db)
        supabase_service = SupabaseService()

        for row in preview_data:
            # Get product name from various possible keys
            product_name = (
                row.get('product_name') or
                row.get('å•†å“å') or
                row.get('å“å') or
                row.get('è£½å“å') or
                ''
            )

            # å•†å“ç•ªå·ï¼ˆSKUï¼‰ã‹ã‚‰å–å¾—ï¼ˆAmazonã®å ´åˆã¯ã“ã‚ŒãŒãƒ‡ã‚¶ã‚¤ãƒ³ç•ªå·ï¼‰
            product_code = (
                row.get('å•†å“ç•ªå·') or
                row.get('å•†å“ç®¡ç†ç•ªå·') or
                row.get('SKU') or
                row.get('sku') or
                row.get('å•†å“ã‚³ãƒ¼ãƒ‰') or
                row.get('ç®¡ç†ç•ªå·') or
                row.get('product_code') or
                ''
            )

            # å•†å“ã‚¿ã‚¤ãƒ—ã®æŠ½å‡ºï¼ˆå„ªå…ˆé †ä½é †ï¼‰
            product_type_from_design = None
            design_no = None

            # ãƒ‡ãƒãƒƒã‚°: å•†å“ç•ªå·ã‚’ç¢ºèª
            if product_code:
                logger.info(f"ğŸ” å•†å“ç•ªå·å–å¾—: {product_code.strip()[:50]}...")
            else:
                logger.info(f"âš ï¸ å•†å“ç•ªå·ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            # 1. å•†å“ç•ªå·ï¼ˆSKUï¼‰â†’ ãƒ­ãƒ¼ã‚«ãƒ«DBï¼ˆãƒ‡ã‚¶ã‚¤ãƒ³ãƒã‚¹ã‚¿ãƒ¼ï¼‰æ¤œç´¢ï¼ˆæœ€å„ªå…ˆï¼‰
            if product_code and product_code.strip():
                logger.info(f"ğŸ” å•†å“ç•ªå·ã§ãƒ­ãƒ¼ã‚«ãƒ«DBæ¤œç´¢é–‹å§‹: {product_code.strip()}")
                product_type_from_design = device_detector.get_product_type_by_sku(product_code.strip())
                if product_type_from_design:
                    design_no = product_code.strip()
                    row['extracted_memo'] = product_type_from_design
                    row['design_number'] = design_no
                    row['product_type_source'] = 'local_db_sku'
                    logger.info(f"âœ… ãƒ­ãƒ¼ã‚«ãƒ«DBï¼ˆSKUï¼‰ã‹ã‚‰å•†å“ã‚¿ã‚¤ãƒ—å–å¾—: {design_no} â†’ {product_type_from_design}")

            # 2. å•†å“ç•ªå·ï¼ˆSKUï¼‰â†’ Supabaseæ›–æ˜§æ¤œç´¢
            if not product_type_from_design and product_code and product_code.strip():
                logger.info(f"ğŸ” å•†å“ç•ªå·ã§Supabaseæ›–æ˜§æ¤œç´¢: {product_code.strip()}")
                product_type_from_design = supabase_service.fuzzy_search_product_type(product_code.strip())
                if product_type_from_design:
                    design_no = product_code.strip()
                    row['extracted_memo'] = product_type_from_design
                    row['design_number'] = design_no
                    row['product_type_source'] = 'supabase_fuzzy'
                    logger.info(f"âœ… Supabaseæ›–æ˜§æ¤œç´¢ã‹ã‚‰å•†å“ã‚¿ã‚¤ãƒ—å–å¾—: {design_no} â†’ {product_type_from_design}")

            # 2.5. å•†å“ç•ªå·ï¼ˆãƒ‡ã‚¶ã‚¤ãƒ³ç•ªå·ï¼‰â†’ æ¥½å¤©SKUç®¡ç†ã‚·ã‚¹ãƒ†ãƒ DB
            if not product_type_from_design and product_code and product_code.strip():
                if hasattr(device_detector, 'rakuten_sku') and device_detector.rakuten_sku:
                    logger.info(f"ğŸ” æ¥½å¤©SKUç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§å•†å“ã‚¿ã‚¤ãƒ—æ¤œç´¢: {product_code.strip()}")
                    product_type_from_rakuten = device_detector.rakuten_sku.get_product_type_by_design_number(product_code.strip())
                    if product_type_from_rakuten:
                        design_no = product_code.strip()
                        row['extracted_memo'] = product_type_from_rakuten
                        row['design_number'] = design_no
                        row['product_type_source'] = 'rakuten_sku_db'
                        product_type_from_design = product_type_from_rakuten
                        logger.info(f"âœ… æ¥½å¤©SKUç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å•†å“ã‚¿ã‚¤ãƒ—å–å¾—: {design_no} â†’ {product_type_from_rakuten}")

            # 3. å•†å“ç•ªå·ï¼ˆSKUï¼‰â†’ å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰äºˆæ¸¬
            if not product_type_from_design and product_code and product_code.strip():
                logger.info(f"ğŸ” å•†å“ç•ªå·ã§å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³äºˆæ¸¬: {product_code.strip()}")
                prediction = product_type_learning_service.predict_product_type(product_code.strip())
                if prediction:
                    product_type_from_design, confidence, method = prediction
                    design_no = product_code.strip()
                    row['extracted_memo'] = product_type_from_design
                    row['design_number'] = design_no
                    row['product_type_source'] = method
                    logger.info(f"âœ… å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰å•†å“ã‚¿ã‚¤ãƒ—äºˆæ¸¬: {design_no} â†’ {product_type_from_design} (ä¿¡é ¼åº¦: {confidence:.2f})")

            # 4. å•†å“å â†’ ãƒ‡ã‚¶ã‚¤ãƒ³ç•ªå·æŠ½å‡º â†’ ãƒ‡ã‚¶ã‚¤ãƒ³ãƒã‚¹ã‚¿ãƒ¼æ¤œç´¢
            if not product_type_from_design and product_name:
                logger.info(f"ğŸ” å•†å“åã‹ã‚‰ãƒ‡ã‚¶ã‚¤ãƒ³ç•ªå·æŠ½å‡º: {product_name[:30]}...")
                product_type_from_design, design_no = device_detector.get_product_type_from_design(product_name)
                if product_type_from_design:
                    row['extracted_memo'] = product_type_from_design
                    row['design_number'] = design_no
                    row['product_type_source'] = 'design_master_name'
                    logger.info(f"âœ… å•†å“åã‹ã‚‰å•†å“ã‚¿ã‚¤ãƒ—å–å¾—: {design_no} â†’ {product_type_from_design}")

            # 5. å•†å“å â†’ å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰äºˆæ¸¬
            if not product_type_from_design and product_name:
                logger.info(f"ğŸ” å•†å“åã§å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³äºˆæ¸¬: {product_name[:30]}...")
                prediction = product_type_learning_service.predict_product_type(product_name)
                if prediction:
                    product_type_from_design, confidence, method = prediction
                    row['extracted_memo'] = product_type_from_design
                    row['design_number'] = design_no if design_no else ''
                    row['product_type_source'] = method
                    logger.info(f"âœ… å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå•†å“åï¼‰ã‹ã‚‰å•†å“ã‚¿ã‚¤ãƒ—äºˆæ¸¬: {product_name[:30]}... â†’ {product_type_from_design} (ä¿¡é ¼åº¦: {confidence:.2f})")

            # 6. æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹å•†å“ã‚¿ã‚¤ãƒ—æŠ½å‡ºï¼ˆæœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not product_type_from_design and product_name:
                logger.info(f"ğŸ” æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹å•†å“ã‚¿ã‚¤ãƒ—æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
                extracted_keywords = ImportService._extract_product_keywords(product_name)
                row['extracted_memo'] = extracted_keywords
                row['design_number'] = design_no if design_no else ''
                row['product_type_source'] = 'regex'
                logger.info(f"âœ… æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹å•†å“ã‚¿ã‚¤ãƒ—: {extracted_keywords}")
            elif not product_type_from_design:
                row['extracted_memo'] = ''
                row['design_number'] = ''
                row['product_type_source'] = 'not_found'
                logger.warning(f"âš ï¸ å•†å“ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {product_name[:50] if product_name else 'N/A'}...")

            # æ©Ÿç¨®æ¤œå‡ºï¼ˆå„ªå…ˆé †ä½é †ï¼‰
            device = None
            method = None
            brand = None

            # 1. ãƒ‡ã‚¶ã‚¤ãƒ³ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰æ©Ÿç¨®ã‚’å–å¾—ï¼ˆå•†å“ç•ªå·ã‹ã‚‰ï¼‰
            if product_code and product_code.strip():
                device_from_design = supabase_service.get_device_by_design(product_code.strip())
                if device_from_design:
                    device = device_from_design
                    method = 'design_master'
                    # ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’æŠ½å‡ºï¼ˆæœ€åˆã®å˜èªï¼‰
                    brand = device.split()[0] if ' ' in device else device.split('/')[0] if '/' in device else None
                    logger.info(f"ğŸ“± ãƒ‡ã‚¶ã‚¤ãƒ³ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰æ©Ÿç¨®å–å¾—: {product_code.strip()} â†’ {device}")

            # 2. å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ©Ÿç¨®ã‚’äºˆæ¸¬ï¼ˆå•†å“åã‹ã‚‰ï¼‰
            if not device and product_name:
                prediction = device_learning_service.predict_device(product_name)
                if prediction:
                    device, brand, confidence, method = prediction
                    logger.info(f"ğŸ¯ å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ©Ÿç¨®äºˆæ¸¬: {product_name[:30]}... â†’ {device} (ä¿¡é ¼åº¦: {confidence:.2f})")

            # 3. é€šå¸¸ã®æ©Ÿç¨®æ¤œå‡ºï¼ˆé¸æŠè‚¢åˆ—ã€æ©Ÿç¨®å°‚ç”¨åˆ—ã€å•†å“ååˆ—ã€ãã®ä»–ã®åˆ—ï¼‰
            if not device:
                device, method, brand = device_detector.detect_device_from_row(row)

            row['detected_device'] = device if device else 'æœªæ¤œå‡º'
            row['device_detection_method'] = method if device else 'not_found'
            row['detected_brand'] = brand if brand else 'æœªæ¤œå‡º'

            # ã‚µã‚¤ã‚ºæŠ½å‡ºï¼ˆæ‰‹å¸³å‹ã‚«ãƒãƒ¼ã®å ´åˆã®ã¿ï¼‰
            product_name = (
                row.get('product_name') or
                row.get('å•†å“å') or
                row.get('å“å') or
                row.get('è£½å“å') or
                ''
            )
            product_type = row.get('extracted_memo', '')

            # æ‰‹å¸³å‹ã‚«ãƒãƒ¼ã®å ´åˆã®ã¿ã‚µã‚¤ã‚ºã‚’æŠ½å‡º
            if product_type and 'æ‰‹å¸³' in product_type:
                size = None
                size_method = None

                if product_name:
                    # 1. å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰äºˆæ¸¬ï¼ˆæœ€å„ªå…ˆï¼‰
                    prediction = size_learning_service.predict_size(product_name, device_name=device)
                    if prediction:
                        size, confidence, size_method = prediction
                        logger.info(f"ğŸ“ å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã‚µã‚¤ã‚ºäºˆæ¸¬: {product_name[:30]}... â†’ {size} (ä¿¡é ¼åº¦: {confidence:.2f})")

                    # 2. å•†å“å±æ€§ï¼ˆ_i6, _L ãªã©ï¼‰ã‹ã‚‰æŠ½å‡º
                    if not size:
                        size, size_method = device_detector.extract_size_from_product_name(
                            product_name,
                            product_type,
                            brand=brand,
                            device=device,
                            row=row  # é¸æŠè‚¢åˆ—ã‹ã‚‰ã®æŠ½å‡ºã‚‚å¯èƒ½ã«ã™ã‚‹
                        )
                        logger.info(f"ğŸ“ å•†å“å±æ€§ã‹ã‚‰ã‚µã‚¤ã‚ºæŠ½å‡º: {product_name[:30]}... â†’ ã‚µã‚¤ã‚º={size}, æ–¹æ³•={size_method}")

                    row['detected_size'] = size if size else '-'
                    row['size_detection_method'] = size_method if size else 'not_found'
                else:
                    row['detected_size'] = '-'
                    row['size_detection_method'] = 'not_found'
            else:
                # ãƒãƒ¼ãƒ‰ã‚±ãƒ¼ã‚¹ç­‰ã€æ‰‹å¸³å‹ä»¥å¤–ã¯ã‚µã‚¤ã‚ºæŠ½å‡ºã—ãªã„
                row['detected_size'] = '-'
                row['size_detection_method'] = 'not_applicable'
                if product_type:
                    logger.info(f"â„¹ï¸ ã‚µã‚¤ã‚ºæŠ½å‡ºã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ‰‹å¸³å‹ä»¥å¤–ï¼‰: å•†å“ã‚¿ã‚¤ãƒ—={product_type}")

        # Add extracted_memo, detected_brand, detected_device, detected_size to columns if not present
        columns_with_extras = parse_result.columns.copy()
        if 'extracted_memo' not in columns_with_extras:
            columns_with_extras.append('extracted_memo')
        if 'detected_brand' not in columns_with_extras:
            columns_with_extras.append('detected_brand')
        if 'detected_device' not in columns_with_extras:
            columns_with_extras.append('detected_device')
        if 'detected_size' not in columns_with_extras:
            columns_with_extras.append('detected_size')

        return ParsePreviewResponse(
            success=True,
            columns=columns_with_extras,
            data=preview_data,
            row_count=len(preview_data),
            total_rows_estimate=parse_result.row_count,
            warnings=parse_result.warnings,
            errors=parse_result.errors,
            metadata=parse_result.metadata
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Preview failed: {str(e)}"
        )


@router.post("/jobs", response_model=ImportJobResponse, status_code=status.HTTP_201_CREATED)
async def create_import_job(
    request: ImportJobCreateRequest,
    db: Session = Depends(get_db)
):
    """
    Create import job and start processing asynchronously.
    """
    try:
        # Verify upload exists
        upload_dir = Path(tempfile.gettempdir()) / "accusync_uploads"
        file_ext = Path(request.filename).suffix.lower()
        file_path = upload_dir / f"{request.upload_id}{file_ext}"

        if not file_path.exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Upload not found or expired"
            )

        # Create import job record
        job = ImportJob(
            upload_id=request.upload_id,
            filename=request.filename,
            file_type=request.file_type,
            status=ImportJobStatus.PENDING,
            total_rows=0,
            processed_rows=0,
            error_count=0
        )
        db.add(job)
        db.commit()
        db.refresh(job)

        # Start async processing
        process_file_import.delay(
            job_id=job.id,
            file_path=str(file_path),
            filename=request.filename,
            apply_ai_mapping=request.apply_ai_mapping,
            apply_quality_check=request.apply_quality_check,
            target_fields=request.target_fields,
            parser_options=request.parser_options
        )

        return ImportJobResponse.from_orm(job)

    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Job creation failed: {str(e)}"
        )


@router.get("/jobs/{job_id}", response_model=ImportJobResponse)
async def get_import_job(
    job_id: int,
    db: Session = Depends(get_db)
):
    """
    Get import job status and details.
    """
    job = db.query(ImportJob).filter(ImportJob.id == job_id).first()

    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Import job {job_id} not found"
        )

    return ImportJobResponse.from_orm(job)


@router.get("/jobs", response_model=List[ImportJobResponse])
async def list_import_jobs(
    skip: int = 0,
    limit: int = 100,
    status: ImportJobStatus = None,
    db: Session = Depends(get_db)
):
    """
    List import jobs with optional filtering.
    """
    query = db.query(ImportJob)

    if status:
        query = query.filter(ImportJob.status == status)

    jobs = query.order_by(ImportJob.created_at.desc()).offset(skip).limit(limit).all()

    return [ImportJobResponse.from_orm(job) for job in jobs]


@router.post("/jobs/{job_id}/import", response_model=ImportDataResponse)
async def import_data(
    job_id: int,
    request: ImportDataRequest,
    db: Session = Depends(get_db)
):
    """
    Import parsed data from job into database.
    """
    from app.services.import_service import ImportService

    job = db.query(ImportJob).filter(ImportJob.id == job_id).first()

    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Import job {job_id} not found"
        )

    if job.status != ImportJobStatus.COMPLETED:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Job not ready for import (status: {job.status})"
        )

    try:
        # Extract data from job result
        data = job.result_data.get('data_sample', []) if job.result_data else []

        if not data:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No data available for import"
            )

        # Import data using ImportService
        result = ImportService.import_order_data(
            db=db,
            data=data,
            column_mapping=request.column_mapping,
            issuer_id=request.issuer_id,
            customer_id=request.customer_id
        )

        return ImportDataResponse(
            success=result['success'],
            imported_rows=result['imported_rows'],
            skipped_rows=result['skipped_rows'],
            error_rows=result['error_rows'],
            warnings=result['warnings'],
            errors=result['errors']
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Import failed: {str(e)}"
        )


@router.delete("/jobs/{job_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_import_job(
    job_id: int,
    db: Session = Depends(get_db)
):
    """
    Delete import job and associated data.
    """
    job = db.query(ImportJob).filter(ImportJob.id == job_id).first()

    if not job:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Import job {job_id} not found"
        )

    # Clean up uploaded file
    try:
        upload_dir = Path(tempfile.gettempdir()) / "accusync_uploads"
        file_ext = Path(job.filename).suffix.lower()
        file_path = upload_dir / f"{job.upload_id}{file_ext}"
        if file_path.exists():
            os.remove(file_path)
    except Exception:
        pass

    db.delete(job)
    db.commit()


@router.get("/mapping/fields")
async def get_standard_fields():
    """
    Get list of standard fields for mapping.
    """
    return {
        "fields": [field.dict() for field in STANDARD_FIELDS]
    }


@router.post("/mapping/suggest", response_model=AutoMappingResult)
async def suggest_column_mapping(
    columns: List[str],
    db: Session = Depends(get_db)
):
    """
    Suggest automatic column mapping based on source column names.

    Args:
        columns: List of source column names

    Returns:
        AutoMappingResult with suggested mappings
    """
    try:
        result = auto_map_columns(columns)
        return result
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Mapping suggestion failed: {str(e)}"
        )
